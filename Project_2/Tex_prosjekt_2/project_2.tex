\documentclass[%
reprint,
amsmath,amssymb,
aps,
]{revtex4-1}
\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{caption}

\makeatletter
\newcommand*{\rom}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother

\begin{document}
\title{Eigenvalue problems}
\author{Torstein S. Ã˜lberg, Ada M. Veddegjerde and Oline A. Ranum}
\affiliation{%
 \textnormal{Universitetet i Oslo, Institutt for fysikk}\\
 olinear@student.matnat.uio.no : torsteol@student.matnat.uio.no
}
\date{\today}


\begin{abstract}
	The following experiment was undertaken as
\end{abstract}
\maketitle

\section*{Introduction}


\section*{Theory}

\subsection*{Eigensystem solvers} \noindent 
For a discretized systems it is possible to directly solve eigenvalue problems using similarity transformations. For instance using the Jacobi rotation method. The jacobi method transforms the matrix into a tridiagonal form using Housholder's algorithm. Given a symmetric matrix $A\in \mathbb{R}^{n\times n}$, we know that there exsists a orthogonal matrix S so that 
\begin{equation*}
	\mathbf{S}^T\mathbf{A}\mathbf{S} = \mathbf{D}
\end{equation*}
where 
\begin{equation*}
	\mathbf{D} = \begin{bmatrix}
	\lambda_1 & 0 & 0 & \dots & 0 \\
	0 & \lambda_2 & 0 & \dots & 0 \\
	\vdots & \vdots & \vdots &\vdots&\vdots\\
	0 & 0 & 0 &\dots & \lambda_n
	\end{bmatrix}
\end{equation*}
and $\lambda_i$ are the eigenvalues of $A$. Thus one can preform a series of similarity transformations on $\mathbf{A}$ in order to reduce it to a diagonal form. Matrix $\mathbf{B}$ is said to be a similarity transform of $\mathbf{A}$ if 
\begin{equation} \label{eq2}
	\mathbf{B} = \mathbf{S}^T\mathbf{A}\mathbf{S}, \hspace{3mm} \textnormal{where} \hspace{3mm} \mathbf{S}^T\mathbf{S} = \mathbf{S}^{-1}\mathbf{S} = \mathbf{I}
\end{equation}
A similarity transform yields a matrix with the same eigenvalues, but in general different eigenvectors. \\
Initially, one has the eigenvalue problem 
\begin{equation}
	\mathbf{A}\mathbf{x} = \lambda\mathbf{x}
\end{equation}
and a similarity matrix as defined in \ref{eq2}. One then subsequently apply a similarity transform so that 
\begin{equation}
	\mathbf{S}_N^T...\mathbf{S}_1^T\mathbf{A}\mathbf{S}_1...\mathbf{S}_N = \mathbf{D}
\end{equation}
Where the diagonal elements of $\mathbf{D}$ are the eigenvalues of $\mathbf{A}$, this method is better known as Jacobi's method [M. Jensen].

\subsubsection*{Jacobi's method}  \noindent 
Jacobi's method conciders an $n\times n$ orthogonal transformation matrix on the form 
\begin{equation*}
\mathbf{S} = \begin{bmatrix}
1 & 0  & \dots & 0 &\vdots &0 \\
0 & 1  & \dots & 0 & \vdots & 0 \\
\vdots & \vdots &\vdots&\vdots&\vdots&\vdots\\
0 & 0 & \cos{\theta} & 0 &\vdots & \sin{\theta} \\
0  & 0 & 0 & 1 &\vdots & 0 \\
 \vdots & \vdots &\vdots&\vdots&\vdots&\vdots\\
 0 & 0 &\dots & -\sin{\theta} & \vdots & \cos{\theta}
\end{bmatrix}
\end{equation*}
where $\mathbf{S} = \mathbf{S}^-1$. This matrix makes a plane rotation of an angle $\theta$ in the Euclidean n-dimensional space. The similarity matrix is defined through the quantities $\tan\theta = t= s/c$, with $s=\sin\theta$ and $c=\cos\theta$ and
\begin{equation*}\cot 2\theta=\tau = \frac{a_{ll}-a_{kk}}{2a_{kl}}.
\end{equation*}
where $a_{kl}$ should be the largest off-diagonal matrix element in order to ensure that all other off-diagonal elements converges towards zero. The identity $\cot 2\theta=1/2(\cot \theta-\tan\theta)$ leads to the quadratic equation
\begin{equation*}
t^2+2\tau t-1= 0,
\end{equation*}
resulting in
\begin{equation*}
t = -\tau \pm \sqrt{1+\tau^2},
\end{equation*}
Where we choose the sign that minimizes t. Then $c$ and $s$ are easily obtained via
\begin{equation}\label{sc}
c = \frac{1}{\sqrt{1+t^2}} \hspace{5mm}  \textnormal{and} \hspace{5mm} s=tc
\end{equation}  

\subsection{The analytical solution of a tridiagonal matrix}
A tridiagonal matrix has analytical eigenpairs, with eigenvalues gives as 
\begin{equation}\label{analytical}
	\lambda_j = d+2a\cos{(\frac{j\pi}{N+1})} \hspace{0.1cm} j=1,2,\dots N-1
\end{equation}


\subsection{The Harmonic Oscillator} \noindent 
The classical harmonic oscillator is described by the following equation of motion
\begin{equation*}
	\gamma \frac{d^2 u(x)}{dx^2} = -F u(x)
\end{equation*}

where $u(x)$ is the vertical displacement of the system in the $y$ direction. 

\subsection{Unitary transformations} \noindent 
A unitary transformation is a transformation that preserves the orthogonality and inner product. Given a basis of vectors $\vec{v_i}$ ,
\begin{equation*}
	\mathbf{v}_i = \begin{bmatrix} v_{i1} \\ \dots \\ \dots \\v_{in} \end{bmatrix}
\end{equation*}
where the basis is orthogonal 
\begin{equation*}
	\mathbf{v}_j^T\mathbf{v}_i = \delta_{ij}
\end{equation*}
Given the unitary transformation 
\begin{equation*}
	\mathbf{w}_i=\mathbf{U}\mathbf{v}_i
\end{equation*}
It can be shown that the transformation preserves orthogonality
\begin{align*}
	\mathbf{w}=\mathbf{U}\mathbf{v} &\implies \mathbf{w}^T=\mathbf{v}^T\mathbf{U}^T \\ &\implies \mathbf{w}^T\mathbf{w} = \mathbf{v}^T\mathbf{U}^T\mathbf{U}\mathbf{v} = \mathbf{v}^T\mathbf{v} = \delta
\end{align*}
To show that it preserves the inner product we define 
\begin{align*}
	\mathbf{a} &= U\mathbf{b} \\
	\rightarrow \mathbf{a} &= \mathbf{b}^TU^T \\
	&\textnormal{We then take the inner product:} \\
	 \mathbf{a}^T\mathbf{w} &= \mathbf{b}^TU^TU\mathbf{v} = \mathbf{b}^T\mathbf{v}
\end{align*}
Where we have showed that the unitary transformation does not affect the inner product. 

\newpage.
\newpage 

\section*{Method}
\subsection{Jacobi's method for Eigensolvers}
\noindent When implementing Jacobi's method we apply the following algorithm to a matrix A:
\begin{align*}
	&b_{ii} = a_{ii} \hspace{2mm}& i\not=k &\hspace{2mm} i\not=l\\
	&b_{ik} = a_{ik}\cos\theta -a_{il}\sin\theta \hspace{2mm}& i\not=k &\hspace{2mm} i\not=l \\
	&b_{il}  = a_{il}\cos\theta -a_{ik}\sin\theta \hspace{2mm}& i\not=k &\hspace{2mm} i\not=l\\
	&b_{kk} = a_{kk}\cos^2\theta -2a_{kl}\cos\theta\sin\theta +a_{ll}\sin^2\theta&&\\
	&b_{ll} = a_{ll}\cos^2\theta -2a_{kl}\cos\theta\sin\theta +a_{kk}\sin^2\theta&& \\
	&b_{kl} = (a_{kk}-a_{ll})\cos\theta\sin\theta+a_{kl}(\cos^2\theta -\sin^2\theta)\\
\end{align*}
where $c = \cos\theta$ and $s = sin\theta$ are defined as in equation \ref{sc}. The system of iterations are implemented in a while loop that keeps track of the norm of all the off diagonal elements. We state that when this sum is belove a threshold of $10^{-10}$, we have reduced the matrix to a diagonal form. From this form we read the eigenvalues of the matrix $\hat{D}$ across the diagonal. \\
In order to ensure that we obtained accurate results, we compare the egeinvalues produced by Jacobis method with the analytical solution produced by equation \ref{analytical}. To do so, we use an initial tridiagonal matrix where all off-diagonal are equal, and all elements across the diagonal are equal. \\
To look closer at the efficiency of the method, we compare the results agains the armadillo function for solving eigenvalue problems. We look at the time both algorithm uses, as well as the proximity to the analytical results. We test for N = 20 grid points for a 2D tridiagonal matrix.  
\newpage.
\newpage 

\section*{Results}
\begin{figure}[!h]
	\begin{tabular} {|c|c|c|c|}
		\hline
		Run & \# Transforms & Time Jacobi [s] & Time armadillo [s] \\ 
		\hline
		&&& \\ 
		1 & 19286 & 3.687817             & 0.002251            \\ 
		2 & 19286 & 3.520625             & 0.001646            \\ 
		3 & 19286 & 3.53273             & 0.001579            \\ 
		4 & 19286 & 3.51314             & 0.001565            \\ 
		5 & 19286 & 3.536378             & 0.001629            \\ 
		6 & 19286 & 3.560997             & 0.001663            \\ 
		7 & 19286 & 3.567837             & 0.001561            \\ 
		8 & 19286 & 4.936219             & 0.001647            \\ 
		9 & 19286 & 4.872346             & 0.003301            \\ 
		10 & 19286 & 3.865138             & 0.001855            \\ 
		\hline 
		& & &\\
		Average &&& \\ 
		\hline
	\end{tabular}
	\label{tab1}
	\captionof{table}[foo]{Time estimations of the Jacobi and the armadillo eig\_sym solvers to find eigenvalues of a 100$\times$100 matrix. Included the number of similarity transformations nesecarry during this run of the Jacobi method.}
\end{figure}

\section*{Discussion} 

\section*{Conclusion}


\newpage .
\newpage 
\onecolumngrid
\section*{Bibliography}
\noindent $[1]$ \\ 
$[2]$
\section*{Appendix A}
For the code used for calculation our results, visit
\url{https://github.com/OlineRanum/FYS3150_Project_1}

\end{document}